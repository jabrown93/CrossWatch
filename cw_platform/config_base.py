# cw_platform/config_base.py
# configuration management base.
# Copyright (c) 2025-2026 CrossWatch / Cenodude (https://github.com/cenodude/CrossWatch)
from __future__ import annotations

import copy
import json
import os
from collections.abc import Iterable
from pathlib import Path
from typing import Any, cast


def _current_version_norm() -> str:
    try:
        from api.versionAPI import CURRENT_VERSION as _V
        raw = str(_V)
    except Exception:
        raw = (os.getenv("APP_VERSION") or "v0.7.0").strip()
    return raw[1:] if raw.lower().startswith("v") else raw


def CONFIG_BASE() -> Path:
    env = os.getenv("CONFIG_BASE")
    if env:
        return Path(env)

    if Path("/app").exists():
        # In container image mount /config as a writable volume
        return Path("/config")
    return Path(__file__).resolve().parents[1]


CONFIG: Path = CONFIG_BASE()
CONFIG.mkdir(parents=True, exist_ok=True)

# Default config
DEFAULT_CFG: dict[str, Any] = {
    # --- Providers -----------------------------------------------------------
    "plex": {
        "server_url": "",                               # http(s)://host:32400 (required for sync & watcher).
        "verify_ssl": True,                             # Verify TLS certificates
        "webhook_secret": "",                           # Shared secret for X-Plex-Signature verification
        "account_token": "",                            # Plex token (a.k.a. authentication token).
        "pms_token": "",                                # PMS resource token for the selected server
        "client_id": "",                                # Set by PIN login; reused for headers.
        "machine_id": "",                               # PMS machineIdentifier (UUID).
        "username": "",                                 # Preferred Plex Home user/profile.
        "account_id": "",                               # Server-local accountID (int) for the selected user. 
        "home_pin": "",                                 # Plex Home PIN for the selected profile (optional).
        "timeout": 10.0,                                # Optional HTTP timeout (seconds).
        "max_retries": 3,                               # Optional retry budget.
        "fallback_GUID": False,                         # Opt-in GUID/Discover fallback for missing library items (history/ratings)

        "scrobble": {
            "libraries": [],                            # Whitelist of library IDs for scrobble (webhook/watch); empty = all
        },

        "history": {
            "libraries": [],                            # Whitelist of library GUIDs; empty = all
            "include_marked_watched": True,             # Include items manually marked as watched in Plex
        },
        "ratings": {
            "libraries": [],                            # Whitelist of library GUIDs; empty = all
        },

        # Ratings / History
        "rating_workers": 12,                           # Parallel workers for Plex ratings indexing. 12–16 is ideal on a local NAS.
        "history_workers": 12,                          # Parallel workers for Plex history indexing. 12–16 is ideal on a local NAS.

        # Watchlist via Discover (with PMS fallback toggle)
        "watchlist_allow_pms_fallback": False,          # Allow PMS watchlist fallback when needed. Keep False for strict Discover-only behavior.
        "watchlist_page_size": 100,                     # Discover page size (100-200). Higher = faster, but more risk of 504 timeouts.
        "watchlist_query_limit": 25,                    # Max Discover search results per query (10–25). Lower = faster, 25 = safer.
        "watchlist_write_delay_ms": 0,                  # Optional pacing between Discover writes; set 50–150 if you hit 429/5xx.
        "watchlist_title_query": True,                  # Use title/slug tokens for Discover candidate fetching (Discover is text-only).
        "watchlist_use_metadata_match": True,           # Try METADATA /library/metadata/matches with imdb-/tmdb-/tvdb- first; fallback to Discover.
        "watchlist_guid_priority": [                    # GUID resolution order (first match wins).
            "tmdb", "imdb", "tvdb",
            "agent:themoviedb:en", "agent:themoviedb", "agent:imdb"
        ],
    },

    "simkl": {
        "access_token": "",                             # OAuth2 access token
        "refresh_token": "",                            # OAuth2 refresh token
        "token_expires_at": 0,                          # Epoch when access_token expires
        "client_id": "",                                # From your Simkl app
        "client_secret": "",                            # From your Simkl app
        "date_from": "",                                # YYYY-MM-DD (optional start date for full sync)
    },
    
    "anilist": {
        "client_id": "",                                # From your AniList app
        "client_secret": "",                            # From your AniList app
        "access_token": "",                             # OAuth access token (saved after auth)
        "user": {},                                     # Viewer object (id/name)
    },

    "mdblist": {
        "api_key": "",                                  # Your MDBList API key
        "timeout": 10,                                  # HTTP timeout (seconds)
        "max_retries": 3,                               # Retry budget

        # Watchlist
        "watchlist_shadow_ttl_hours": 0,                # Shadow TTL (hours); 0 = disabled
        "watchlist_shadow_validate": True,              # Validate shadow on every run
        "watchlist_page_size": 200,                     # GET page size for /watchlist/items
        "watchlist_batch_size": 100,                    # Batch size for add/remove writes
        "watchlist_freeze_details": True,               # Store extra details for "not_found" freezes

        # Ratings
        "ratings_per_page": 200,                        # Items per page when indexing
        "ratings_max_pages": 50,                        # Max pages to fetch (safety cap)
        "ratings_chunk_size": 25,                       # Batch size for POST/REMOVE
        "ratings_write_delay_ms": 600,                  # Optional pacing between writes
        "ratings_max_backoff_ms": 8000,                 # Max backoff time for retries
        "ratings_since": "1970-01-01T00:00:00Z",        # First-run baseline; watermark overrides after

        # History
        "history_per_page": 1000,                       # Items per page for /sync/watched delta
        "history_max_pages": 250,                       # Max pages to fetch (safety cap)
        "history_chunk_size": 25,                       # Batch size for watched/unwatched writes
        "history_write_delay_ms": 600,                  # Optional pacing between writes
        "history_max_backoff_ms": 8000,                 # Max backoff time for retries
        "history_since": "1970-01-01T00:00:00Z"         # First-run baseline; watermark overrides after
    },
    
     "tautulli": {
         "server_url": "",                              # http(s)://host:8181
         "api_key": "",                                 # Tautulli API key
         "verify_ssl": True,                            # Verify TLS certificates
         "timeout": 10.0,                               # HTTP timeout (seconds)
         "max_retries": 3,                              # Retry budget
         "history": {
             "user_id": "",                             # Optional user filter
             "per_page": 100,                           # Tautulli history page size
             "max_pages": 5000                          # Safety cap
         },
     },

    "trakt": {
        "client_id": "",                                # From your Trakt app
        "client_secret": "",                            # From your Trakt app
        "access_token": "",                             # OAuth2 access token
        "refresh_token": "",                            # OAuth2 refresh token
        "scope": "public",                              # OAuth2 scope (usually "public" or "private")
        "token_type": "Bearer",                         # OAuth2 token type (usually "Bearer")
        "expires_at": 0,                                # Epoch when access_token expires

        "timeout": 10,                                  # HTTP timeout (seconds)
        "max_retries": 5,                               # Retry budget for API calls (429/5xx backoff)

        # Watchlist
        "watchlist_use_etag": True,                     # Use ETag + local shadow to skip unchanged lists
        "watchlist_shadow_ttl_hours": 168,              # Refresh ETag baseline weekly even if 304s keep coming
        "watchlist_batch_size": 100,                    # Chunk size for add/remove to avoid 429/rate spikes
        "watchlist_log_rate_limits": True,              # Log X-RateLimit-* and Retry-After when present
        "watchlist_freeze_details": True,               # Persist last status & ids in freeze store for debugging

        # Ratings
        "ratings_per_page": 100,                        # Items per page when indexing (10–100; clamped to 100)
        "ratings_max_pages": 50,                        # Max pages per type; raise if you have >2k ratings/type
        "ratings_chunk_size": 100,                      # Batch size for POST/REMOVE

        # History
        "history_per_page": 100,                        # Max allowed by Trakt; fastest without spamming
        "history_max_pages": 10000,                     # Safety cap for huge libraries; lower to bound runtime
        "history_unresolved": False,                    # bool, default false (enable the freeze file)
        "history_number_fallback": False,               # episode number fallback (no S/E-based resolution when episode IDs are missing)

        "_pending_device": {
            "user_code": "",                            # Temporary device code state for PIN login
            "device_code": "",                          # Temporary device code state for PIN login
            "verification_url": "https://trakt.tv/activate",
            "interval": 5,                              # Polling interval (seconds)
            "expires_at": 0,                            # Epoch when device_code expires
            "created_at": 0,                            # Epoch when device_code was created
        },
    },
"tmdb_sync": {                                          # Tracker / sync adapter auth (TMDb v3)
    "api_key": "",                                      # v3 API Key (required)
    "session_id": "",                                   # v3 Session ID (filled after Connect)
    "account_id": "",                                   # v3 Account ID (auto-filled)
    "_pending_request_token": "",                       # Temporary token waiting for approval
    "_pending_created_at": 0,                           # Epoch when request token was created
    "timeout": 15.0,
    "max_retries": 3,
},
    "jellyfin": {
        "server": "",                                   # http(s)://host:port (required)
        "access_token": "",                             # Jellyfin access token (required)
        "user_id": "",                                  # Jellyfin userId (required)
        "device_id": "crosswatch",                      # Client device id
        "username": "",                                 # Optional (login username)
        "user": "",                                     # Optional (display name; hydrated after auth)
        "verify_ssl": True,                             # Verify TLS certificates
        "webhook_secret": "",                           # Shared secret for X-CW-Webhook-Secret verification
        "timeout": 15.0,                                # HTTP timeout (seconds)
        "max_retries": 3,                               # Retry budget for API calls

        "scrobble": {
            "libraries": []                             # whitelist of library GUIDs; empty = all
        },

        # Watchlist settings
        "watchlist": {
            "mode": "favorites",                        # "favorites" | "playlist" | "collections"
            "playlist_name": "Watchlist",               # used when mode == "playlist"
            "watchlist_query_limit": 25,                # batch size
            "watchlist_write_delay_ms": 0,              # delay between writes
            "watchlist_guid_priority": [                # id match order
                "tmdb", "imdb", "tvdb",
                "agent:themoviedb:en", "agent:themoviedb", "agent:imdb"
            ]
        },

        # History settings
        "history": {
            "history_query_limit": 25,                  # batch size
            "history_write_delay_ms": 0,                # delay between writes
            "history_guid_priority": [                  # id match order
                "tmdb", "imdb", "tvdb",
                "agent:themoviedb:en", "agent:themoviedb", "agent:imdb"
            ],
            "libraries": []                             # whitelist of library GUIDs (from /api/jellyfin/libraries.key); empty = all
        },

        # Ratings settings
        "ratings": {
            "ratings_query_limit": 2000,                # ratings query limit, default 2000
            "libraries": []                             # whitelist of library GUIDs; empty = all
        },
    },

    "emby": {
        "server": "",                                   # http(s)://host:port (required)
        "access_token": "",                             # Emby access token (required)
        "user_id": "",                                  # Emby userId (required)
        "device_id": "crosswatch",                      # Client device id
        "username": "",                                 # Optional (login username)
        "user": "",                                     # Optional (display name; hydrated after auth)
        "verify_ssl": True,                             # Verify TLS certificates
        "webhook_secret": "",                           # Shared secret for X-CW-Webhook-Secret verification
        "timeout": 15.0,                                # HTTP timeout (seconds)
        "max_retries": 3,                               # Retry budget for API calls

        "scrobble": {
            "libraries": []                             # whitelist of library GUIDs; empty = all
        },

        # Watchlist settings
        "watchlist": {
            "mode": "favorites",                        # "favorites" | "playlist" | "collections"
            "playlist_name": "Watchlist",               # used when mode == "playlist"
            "watchlist_query_limit": 25,                # batch size
            "watchlist_write_delay_ms": 0,              # delay between writes
            "watchlist_guid_priority": [                # id match order
                "tmdb", "imdb", "tvdb",
                "agent:themoviedb:en", "agent:themoviedb", "agent:imdb"
            ]
        },

        # History settings
        "history": {
            "history_query_limit": 25,                  # batch size
            "history_write_delay_ms": 0,                # delay between writes
            "history_guid_priority": [                  # id match order
                "tmdb", "imdb", "tvdb",
                "agent:themoviedb:en", "agent:themoviedb", "agent:imdb"
            ],
            "libraries": []                             # whitelist of library GUIDs (from /api/emby/libraries.key); empty = all
        },

        # Ratings settings
        "ratings": {
            "ratings_query_limit": 2000,                # ratings query limit, default 2000
            "libraries": []                             # whitelist of library GUIDs; empty = all
        },
    },

    "crosswatch": {
        "root_dir":         "/config/.cw_provider",    # Root folder for local provider state
        "enabled":          True,                      # Enable/disable CrossWatch as sync provider
        "retention_days":   30,                        # Snapshot retention in days; 0 = keep forever
        "auto_snapshot":    True,                      # Take snapshot before mutating main JSONs
        "max_snapshots":    64,                        # Max snapshots per feature; 0 = unlimited
        "restore_watchlist": "latest",                 # "", "latest", or specific snapshot name/stem
        "restore_history": "latest",                   # "", "latest", or specific snapshot name/stem
        "restore_ratings": "latest"                    # "", "latest", or specific snapshot name/stem
    },
    
    # --- Meta Providers ------------------------------------------------------
    
    "tmdb": {"api_key": ""},                            # Metadata resolver (TMDb)

    # --- Sync / Orchestrator -------------------------------------------------
    "sync": {
        # Global write gates (pair/feature settings will override these by design):
        "enable_add": True,                             # Allow additions by default
        "enable_remove": False,                         # Safer default: do not remove items unless explicitly enabled
        "one_way_remove_mode": "source_deletes",        # "source_deletes" | "mirror" (mirror = destructive; use with care)


        # Execution behavior:
        "verify_after_write": False,                    # When supported, re-check destination after writes
        "dry_run": False,                               # Plan and log only; do not perform writes
        "drop_guard": False,                            # Guard against sudden inventory shrink (protects from bad/suspect snapshots)
        "allow_mass_delete": True,                      # If False, block large delete plans (e.g., >~10% of baseline)
        "tombstone_ttl_days": 1,                        # How long “observed deletes” (tombstones) stay valid
        "include_observed_deletes": True,               # If False, skip processing “observed deletes” for this run. Delta-trackers (SIMKL) will be turned off to prevent accidental removals

        # Optional high-level two-way defaults (pairs always remain the source of truth for mode):
        "bidirectional": {
            "enabled": False,
            "mode": "two-way",                          # Placeholder default; pairs decide final mode per connection
            "source_of_truth": "",                      # Optional: pick one side as tie-breaker if you enforce strict authority
        },

        # Blackbox (including flapper protection)
        "blackbox": {
            "enabled": True,                            # Turn off to fully disable blackbox logic
            "promote_after": 1,                         # Promote an item to blackbox after N consecutive unresolved/fail events
            "unresolved_days": 0,                       # Minimum unresolved age (days) before it counts (0 = immediate)
            "pair_scoped": True,                        # Track per source-target pair to avoid blocking the same title elsewhere
            "cooldown_days": 30,                        # Auto-prune/decay blackbox entries after this cooldown period
            "block_adds": True,                         # When blackboxed, block planned ADDs for that item
            "block_removes": True,                      # When blackboxed, block planned REMOVEs for that item
        },
    },

    # --- Runtime / Diagnostics ----------------------------------------------
    "runtime": {
        "debug": False,                                 # Extra verbose logging (debug level)
        "debug_http": False,                            # Extra verbose HTTP logging (uvicorn access log)
        "debug_mods": False,                            # Extra verbode MODS logging for Synchronization Providers
        "state_dir": "",                                # Optional override for state dir (defaults to CONFIG/state)  - this will break container setups!
        "telemetry": {"enabled": True},                 # Usage stats

        # progress
        "snapshot_ttl_sec": 300,                        # Reuse snapshots within 5 min
        "apply_chunk_size": 100,                        # Sweet spot for apply chunking
        "apply_chunk_pause_ms": 50,                     # Small pause between chunks
        "apply_chunk_size_by_provider": {               # SIMKL/TRAKT/MDBLIST/ANILIST overrides
            "SIMKL": 500
        },
        
        # suspect guard (shrinking inventories protection)
        "suspect_min_prev": 20,                         # Minimum previous size to enable suspect guard
        "suspect_shrink_ratio": 0.10,                   # Shrink ratio to trigger suspect guard
    },

    # --- Metadata (TMDb resolver) -------------------------------------------
    "metadata": {
        "locale": "en-US",                              # example: "en-US" / "nl-NL"
        "ttl_hours": 6,                                 # Coarse cache TTL
    },

    # --- Scrobble ------------------------------------------------------------
    "scrobble": {
        "enabled": False,                               # Master toggle
        "mode": "watch",                                # "watch" | "webhook"
        "delete_plex": False,                           # Old name but still valid. Auto-remove movies from all your Watchlists, for all media servers
        "delete_plex_types": ["movie"],                 # Old name but still valid. Movie/show/episode

        # Watcher settings
        "watch": {
            "autostart": False,                         # Start watcher on boot if enabled+mode=watch
            "provider": "plex",                         # Active watcher either "plex|emby|Jellyfin" (default: "plex")
            "sink": "",                                 # "trakt" | "simkl" | "mdblist"
            "routes": [],                               # Route-based config (preferred); empty = migrate legacy keys
            "plex_simkl_ratings": False,                # Watch mode: forward Plex ratings to SIMKL
            "plex_trakt_ratings": False,                # Watch mode: forward Plex ratings to Trakt
            "plex_mdblist_ratings": False,              # Watch mode: forward Plex ratings to MDblist
            "pause_debounce_seconds": 5,                # Ignore micro-pauses just after start
            "suppress_start_at": 99,                    # Kill near-end "start" flaps (credits)
            "filters": {
                "username_whitelist": [],               # ["name", "id:123", "uuid:abcd…"]
                "server_uuid": ""                       # Restrict to a specific server
            }
        },

        # Webhook settings
        "webhook": {
            "pause_debounce_seconds": 5,                # Ignore micro-pauses
            "suppress_start_at": 99,                    # Suppress near-end "start" flaps (credits)
            "suppress_autoplay_seconds": 15,            # Plex autoplay when set on 10 sec (increase a few sec)
            "probe_session_progress": True,             # Call GET /status/sessions on your Plex server and match the item by ratingKey/sessionKey
            "plex_trakt_ratings": False,                # Watch mode: forward Plex ratings to Trakt
            # Plex-only filters
            "filters_plex": {
                "username_whitelist": [],               # Restrict accepted Account.title values (empty = allow all)
                "server_uuid": ""                       # Restrict to a specific server
            }
        },

        # Trakt sink rules (progress decisions) used by Trakt|SIMKL|MDblist
        "trakt": {
            "progress_step": 25,                        # Send scrobble progress in % steps
            "stop_pause_threshold": 85,                 # <85% STOP-send as PAUSE (your “watched” bar)
            "force_stop_at": 95,                        # ≥85% always STOP (bypass debounces)
            "regress_tolerance_percent": 5,             # Small progress regress is tolerated
        }
    },

    # --- Scheduling ----------------------------------------------------------
    "scheduling": {
        "enabled": False,                               # Standard scheduler master toggle
        "mode": "every_n_hours",                        # "every_n_hours" | "daily_time"
        "every_n_hours": 12,                            # When mode=every_n_hours, run every N hours (1–12)
        "daily_time": "03:30",                          # When mode=daily_time, run at this time (HH:MM, 24h)
        "advanced": {
            "enabled": False,                           # Advanced scheduler master toggle
            "jobs": [],
        },
    },

    # --- User Interface ------------------------------------------------------
    "ui": {
        "show_watchlist_preview": True,                 # Show Watchlist Preview card on Main tab
        "show_playingcard": True,                       # Show Now Playing card on Main tab
        "show_AI": True,                                # Show ASK AI from GitBook
        "protocol": "http",                             # "http" | "https" (HTTPS uses a self-signed cert by default)
        "tls": {
            "self_signed": True,                        # Auto-generate a self-signed certificate when missing
            "hostname": "localhost",                    # Used for certificate CN/SAN
            "valid_days": 825,                          # Certificate validity (days)
            "cert_file": "",                            # Optional override path to a PEM cert
            "key_file": "",                             # Optional override path to a PEM key
        },
    },

    # --- Local UI Authentication --------------------------------------------
    "app_auth": {
        "enabled": False,
        "username": "",
        "password": {
            "scheme": "pbkdf2_sha256",
            "iterations": 260000,
            "salt": "",
            "hash": "",
        },
        "session": {
            "token_hash": "",
            "expires_at": 0,
        },
        "sessions": [],
        "last_login_at": 0,
    },

    # --- Pairs (UI-driven) ---------------------------------------------------
    "pairs": [],
}


_REDACT = "••••••••"

# Canonical list of secret field paths (each is a tuple of dict keys).
# Used by redact_config() and configAPI to keep a single source of truth.
_SECRET_PATHS: list[tuple[str, ...]] = [
    # Plex
    ("plex", "account_token"),
    ("plex", "pms_token"),
    ("plex", "home_pin"),
    ("plex", "webhook_secret"),
    # SIMKL
    ("simkl", "access_token"),
    ("simkl", "refresh_token"),
    ("simkl", "client_secret"),
    # AniList
    ("anilist", "access_token"),
    ("anilist", "client_secret"),
    # MDBList
    ("mdblist", "api_key"),
    # Tautulli
    ("tautulli", "api_key"),
    # Trakt
    ("trakt", "client_secret"),
    ("trakt", "access_token"),
    ("trakt", "refresh_token"),
    ("trakt", "_pending_device", "user_code"),
    ("trakt", "_pending_device", "device_code"),
    # TMDb sync
    ("tmdb_sync", "api_key"),
    ("tmdb_sync", "session_id"),
    ("tmdb_sync", "_pending_request_token"),
    # TMDb metadata
    ("tmdb", "api_key"),
    # Jellyfin
    ("jellyfin", "access_token"),
    ("jellyfin", "webhook_secret"),
    # Emby
    ("emby", "api_key"),
    ("emby", "access_token"),
    ("emby", "webhook_secret"),
    # App auth
    ("app_auth", "password", "hash"),
    ("app_auth", "password", "salt"),
    ("app_auth", "session", "token_hash"),
]


def _redact_path(d: dict[str, Any], path: tuple[str, ...]) -> None:
    """Walk *path* inside *d* and replace the leaf with _REDACT if truthy."""
    node: Any = d
    for key in path[:-1]:
        if not isinstance(node, dict):
            return
        node = node.get(key)
    if isinstance(node, dict):
        leaf = path[-1]
        if node.get(leaf):
            node[leaf] = _REDACT


def redact_config(cfg: dict[str, Any]) -> dict[str, Any]:
    out: dict[str, Any] = copy.deepcopy(cfg or {})

    for path in _SECRET_PATHS:
        _redact_path(out, path)

    # Variable-length sessions array in app_auth
    sessions = (out.get("app_auth") or {}).get("sessions")
    if isinstance(sessions, list):
        for s in sessions:
            if isinstance(s, dict) and s.get("token_hash"):
                s["token_hash"] = _REDACT

    return out


# Helpers: paths, IO, merging, normalization
def _cfg_file() -> Path:
    return CONFIG / "config.json"


def config_path() -> Path:
    return _cfg_file()


def _read_json(p: Path) -> dict[str, Any]:
    with p.open("r", encoding="utf-8") as f:
        return json.load(f)


def _write_json_atomic(p: Path, data: dict[str, Any]) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    import os as _os, time as _time, secrets, threading

    suffix = f".{_time.time_ns()}.{_os.getpid()}.{threading.get_ident()}.{secrets.token_hex(4)}.tmp"
    tmp = p.with_suffix(suffix)

    with tmp.open("w", encoding="utf-8", newline="\n") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
        f.write("\n")
    tmp.replace(p)


def _deep_merge(base: dict[str, Any], override: dict[str, Any]) -> dict[str, Any]:
    out: dict[str, Any] = copy.deepcopy(base)
    for k, v in (override or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)  # type: ignore[assignment]
        else:
            out[k] = v
    return out


# Feature normalization
_ALLOWED_RATING_TYPES: list[str] = ["movies", "shows", "seasons", "episodes"]
_ALLOWED_RATING_MODES: list[str] = ["only_new", "from_date", "all"]
_ALLOWED_UI_PROTOCOLS: list[str] = ["http", "https"]


def _as_list(value: Any) -> list[str]:
    if value is None:
        return []
    if isinstance(value, str):
        return [value]
    if isinstance(value, Iterable):
        return [str(x) for x in value if isinstance(x, (str, int, float))]
    return []


def _normalize_ratings_feature(val: dict[str, Any]) -> dict[str, Any]:
    v: dict[str, Any] = dict(val or {})
    v["enable"] = bool(v.get("enable", False))
    v["add"] = bool(v.get("add", False))
    v["remove"] = bool(v.get("remove", False))

    # types
    raw_types = _as_list(v.get("types"))
    types = [str(t).strip().lower() for t in raw_types]
    if "all" in types:
        types = list(_ALLOWED_RATING_TYPES)
    else:
        types = [t for t in _ALLOWED_RATING_TYPES if t in types]
        if not types:
            types = ["movies", "shows"]
    v["types"] = types

    mode = str(v.get("mode", "only_new")).strip().lower()
    if mode not in _ALLOWED_RATING_MODES:
        mode = "only_new"
    v["mode"] = mode

    from_date = str(v.get("from_date", "") or "").strip()
    if mode != "from_date":
        from_date = ""
    v["from_date"] = from_date

    return v


def _normalize_features_map(features: dict[str, Any] | None) -> dict[str, Any]:
    f: dict[str, Any] = dict(features or {})
    for name, val in list(f.items()):
        if isinstance(val, bool):
            f[name] = {"enable": bool(val), "add": bool(val), "remove": False}
            continue

        if isinstance(val, dict):
            v: dict[str, Any] = dict(val)
            v.setdefault("enable", True)
            v.setdefault("add", True)
            v.setdefault("remove", False)

            # Ratings has extra fields
            if name == "ratings":
                v = _normalize_ratings_feature(v)
            f[name] = v
            continue

        # Unknown
        f[name] = {"enable": False, "add": False, "remove": False}
    return f


def _ensure_dict(parent: dict[str, Any], key: str) -> dict[str, Any]:
    v = parent.get(key)
    if isinstance(v, dict):
        return cast(dict[str, Any], v)
    d: dict[str, Any] = {}
    parent[key] = d
    return d


def _normalize_tmdb_sync(cfg: dict[str, Any]) -> None:
    t0 = cfg.get("tmdb_sync")
    if isinstance(t0, dict):
        t = t0
    else:
        t = {}
        cfg["tmdb_sync"] = t

    # TMDb sync v3 config
    t["api_key"] = str(t.get("api_key") or "").strip()
    t["session_id"] = str(t.get("session_id") or "").strip()
    t["account_id"] = str(t.get("account_id") or "").strip()
    t["_pending_request_token"] = str(t.get("_pending_request_token") or "").strip()
    try:
        t["_pending_created_at"] = int(t.get("_pending_created_at") or 0)
    except Exception:
        t["_pending_created_at"] = 0

    try:
        t["timeout"] = float(t.get("timeout", 15.0) or 15.0)
    except Exception:
        t["timeout"] = 15.0
    try:
        t["max_retries"] = int(t.get("max_retries", 3) or 3)
    except Exception:
        t["max_retries"] = 3

    # Guard: if api_key is empty, clear derived/session state
    if not t["api_key"]:
        t["session_id"] = ""
        t["account_id"] = ""
        t["_pending_request_token"] = ""
        t["_pending_created_at"] = 0



def _is_hhmm(v: str) -> bool:
    s = (v or "").strip()
    if len(s) != 5 or s[2] != ":":
        return False
    hh, mm = s[:2], s[3:]
    if not hh.isdigit() or not mm.isdigit():
        return False
    try:
        h = int(hh)
        m = int(mm)
    except Exception:
        return False
    return 0 <= h <= 23 and 0 <= m <= 59


def _normalize_scheduling(cfg: dict[str, Any]) -> None:
    s = _ensure_dict(cfg, "scheduling")
    s["enabled"] = bool(s.get("enabled", False))

    mode_raw = str(s.get("mode", "every_n_hours") or "every_n_hours").strip().lower()
    if mode_raw in {"disabled", "off", "none"}:
        mode = "disabled"
    elif mode_raw in {"hourly", "every_hour"}:
        mode = "hourly"
        s["every_n_hours"] = 1
    elif mode_raw in {"daily", "daily_at", "daily_time"}:
        mode = "daily_time"
    elif mode_raw == "every_n_hours":
        mode = "every_n_hours"
    else:
        mode = "every_n_hours"
    s["mode"] = mode

    try:
        n = int(s.get("every_n_hours", 2) or 2)
    except Exception:
        n = 2
    if n < 1:
        n = 1
    if n > 12:
        n = 12
    s["every_n_hours"] = n

    t = str(s.get("daily_time", "03:30") or "03:30").strip()
    if not _is_hhmm(t):
        t = "03:30"
    s["daily_time"] = t

    adv = _ensure_dict(s, "advanced")
    adv["enabled"] = bool(adv.get("enabled", False))

    jobs0 = adv.get("jobs")
    jobs: list[dict[str, Any]] = []
    if isinstance(jobs0, list):
        for it in jobs0:
            if isinstance(it, dict):
                jobs.append(dict(it))

    out: list[dict[str, Any]] = []
    for i, j in enumerate(jobs):
        jid = str(j.get("id") or "").strip()
        if not jid:
            jid = f"job_{i+1}"
        j["id"] = jid

        pair_id = j.get("pair_id")
        if pair_id is None:
            j["pair_id"] = None
        else:
            s_pair = str(pair_id).strip()
            j["pair_id"] = s_pair or None

        at = str(j.get("at") or "").strip()
        if at and not _is_hhmm(at):
            at = ""
        j["at"] = at or None

        after = j.get("after")
        if after is None:
            j["after"] = None
        else:
            a = str(after).strip()
            if a and not _is_hhmm(a):
                a = ""
            j["after"] = a or None

        days0 = j.get("days")
        days: list[int] = []
        if isinstance(days0, list):
            for d in days0:
                try:
                    di = int(d)
                except Exception:
                    continue
                if di < 1 or di > 7:
                    continue
                if di not in days:
                    days.append(di)
        j["days"] = days

        j["active"] = bool(j.get("active", True))
        out.append(j)

    adv["jobs"] = out


def _normalize_ui(cfg: dict[str, Any]) -> None:
    ui = _ensure_dict(cfg, "ui")

    ui["show_watchlist_preview"] = bool(ui.get("show_watchlist_preview", True))
    ui["show_playingcard"] = bool(ui.get("show_playingcard", True))
    ui["show_AI"] = bool(ui.get("show_AI", True))

    protocol = str(ui.get("protocol", "http") or "http").strip().lower()
    if protocol not in _ALLOWED_UI_PROTOCOLS:
        protocol = "http"
    ui["protocol"] = protocol

    tls = _ensure_dict(ui, "tls")
    tls["self_signed"] = bool(tls.get("self_signed", True))

    hostname = str(tls.get("hostname", "localhost") or "localhost").strip()
    tls["hostname"] = hostname or "localhost"

    try:
        valid_days = int(tls.get("valid_days", 825) or 825)
    except Exception:
        valid_days = 825
    if valid_days < 1:
        valid_days = 1
    if valid_days > 3650:
        valid_days = 3650
    tls["valid_days"] = valid_days

    tls["cert_file"] = str(tls.get("cert_file", "") or "").strip()
    tls["key_file"] = str(tls.get("key_file", "") or "").strip()


# Public API
def load_config() -> dict[str, Any]:
    p = _cfg_file()
    user_cfg: dict[str, Any] = {}
    if p.exists():
        try:
            user_cfg = _read_json(p)
        except Exception:
            user_cfg = {}

    cfg = _deep_merge(DEFAULT_CFG, user_cfg)
    cfg.setdefault("version", _current_version_norm())
    _normalize_tmdb_sync(cfg)
    _normalize_scheduling(cfg)
    pairs = cfg.get("pairs")
    if isinstance(pairs, list):
        for it in pairs:
            if isinstance(it, dict):
                it["features"] = _normalize_features_map(it.get("features"))  # type: ignore[arg-type]
    _normalize_ui(cfg)

    # Scrobble watcher: migrate legacy provider/sink into route mode when routes is empty.
    try:
        from providers.scrobble.routes import ensure_routes

        cfg, migrated = ensure_routes(cfg)
        if migrated:
            try:
                save_config(cfg)
            except Exception:
                pass
    except Exception:
        pass

    # Migrate legacy global Trakt history_collection into pair-scoped overrides (pairs > global).
    migrated_trakt_collection = False
    try:
        t = cfg.get("trakt")
        if isinstance(t, dict) and ("history_collection" in t or "history_collection_types" in t):
            enabled = bool(t.pop("history_collection", False))
            raw_types = t.pop("history_collection_types", None)
            allowed = {"movies", "shows"}
            types: list[str] = []
            if isinstance(raw_types, str):
                types = [x.strip().lower() for x in raw_types.split(",") if x and x.strip()]
            elif isinstance(raw_types, list):
                types = [str(x).strip().lower() for x in raw_types if str(x).strip()]
            types = [x for x in types if x in allowed]
            if enabled and not types:
                types = ["movies"]

            pairs = cfg.get("pairs")
            if isinstance(pairs, list):
                for p in pairs:
                    if not isinstance(p, dict):
                        continue
                    src = str(p.get("source") or "").upper().strip()
                    dst = str(p.get("target") or "").upper().strip()
                    if "TRAKT" not in {src, dst}:
                        continue
                    prov = p.get("providers")
                    if not isinstance(prov, dict):
                        prov = {}
                        p["providers"] = prov
                    trp = prov.get("trakt")
                    if not isinstance(trp, dict):
                        trp = {}
                        prov["trakt"] = trp
                    if "history_collection" not in trp:
                        trp["history_collection"] = enabled
                        migrated_trakt_collection = True
                    if enabled and types and "history_collection_types" not in trp:
                        trp["history_collection_types"] = types
                        migrated_trakt_collection = True
            else:
                migrated_trakt_collection = True
    except Exception:
        pass
    if migrated_trakt_collection:
        try:
            save_config(cfg)
        except Exception:
            pass

    return cfg


def save_config(cfg: dict[str, Any]) -> None:
    data: dict[str, Any] = dict(cfg or {})
    data["version"] = _current_version_norm()
    _normalize_tmdb_sync(data)
    _normalize_scheduling(data)
    _normalize_ui(data)
    pairs = data.get("pairs")
    if isinstance(pairs, list):
        for it in pairs:
            if isinstance(it, dict):
                it["features"] = _normalize_features_map(it.get("features"))  # type: ignore[arg-type]

    _write_json_atomic(_cfg_file(), data)
